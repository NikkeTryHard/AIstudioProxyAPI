"""
è¯·æ±‚å¤„ç†å™¨æ¨¡å—
åŒ…å«æ ¸å¿ƒçš„è¯·æ±‚å¤„ç†é€»è¾‘
"""

import asyncio
import json
import os
from asyncio import Event, Future
from typing import (
    Any,
    Callable,
    Dict,
    List,
    Optional,
    Tuple,
    cast,
)

from fastapi import HTTPException, Request
from fastapi.responses import JSONResponse, StreamingResponse
from playwright.async_api import Error as PlaywrightAsyncError
from playwright.async_api import Locator
from playwright.async_api import Page as AsyncPage

# --- browser_utilsæ¨¡å—å¯¼å…¥ ---
from browser_utils import save_error_snapshot
from browser_utils.page_controller import PageController

# --- é…ç½®æ¨¡å—å¯¼å…¥ ---
from config import (
    MODEL_NAME,
    ONLY_COLLECT_CURRENT_USER_ATTACHMENTS,
    SUBMIT_BUTTON_SELECTOR,
)

# --- modelsæ¨¡å—å¯¼å…¥ ---
from models import ChatCompletionRequest, ClientDisconnectedError

from .client_connection import (
    setup_disconnect_monitoring as _setup_disconnect_monitoring,
)
from .client_connection import test_client_connection as _test_client_connection
from .common_utils import random_id as _random_id
from .context_init import initialize_request_context as _init_request_context
from .context_types import RequestContext
from .error_utils import bad_request, client_disconnected, server_error, upstream_error
from .model_switching import analyze_model_requirements as ms_analyze
from .model_switching import handle_model_switching as ms_switch
from .model_switching import handle_parameter_cache as ms_param_cache
from .page_response import locate_response_elements
from .response_generators import gen_sse_from_aux_stream, gen_sse_from_playwright
from .response_payloads import build_chat_completion_response_json

# --- api_utilsæ¨¡å—å¯¼å…¥ ---
from .utils import (
    calculate_usage_stats,
    maybe_execute_tools,
    prepare_combined_prompt,
    use_stream_response,
    validate_chat_request,
)

_initialize_request_context = _init_request_context


async def _analyze_model_requirements(
    req_id: str, context: RequestContext, request: ChatCompletionRequest
) -> RequestContext:
    """ä»£ç†åˆ° model_switching.analyze_model_requirements"""
    return await ms_analyze(req_id, context, request.model or MODEL_NAME, MODEL_NAME)


# ç›´æ¥ä½¿ç”¨å¯¼å…¥çš„å®ç°

# ç›´æ¥ä½¿ç”¨å¯¼å…¥çš„å®ç°


async def _validate_page_status(
    req_id: str, context: RequestContext, check_client_disconnected: Callable
) -> None:
    """éªŒè¯é¡µé¢çŠ¶æ€"""
    page = context["page"]
    is_page_ready = context["is_page_ready"]

    if not page or page.is_closed() or not is_page_ready:
        raise HTTPException(
            status_code=503,
            detail=f"[{req_id}] AI Studio é¡µé¢ä¸¢å¤±æˆ–æœªå°±ç»ªã€‚",
            headers={"Retry-After": "30"},
        )

    check_client_disconnected("Initial Page Check")


async def _handle_model_switching(
    req_id: str, context: RequestContext, check_client_disconnected: Callable
) -> RequestContext:
    """ä»£ç†åˆ° model_switching.handle_model_switching"""
    return await ms_switch(req_id, context)


async def _handle_model_switch_failure(
    req_id: str,
    page: AsyncPage,
    model_id_to_use: str,
    model_before_switch: str,
    logger: Any,
) -> None:
    """å¤„ç†æ¨¡å‹åˆ‡æ¢å¤±è´¥çš„æƒ…å†µ"""
    import server

    logger.warning(f"[{req_id}] âŒ æ¨¡å‹åˆ‡æ¢è‡³ {model_id_to_use} å¤±è´¥ã€‚")
    # å°è¯•æ¢å¤å…¨å±€çŠ¶æ€
    server.current_ai_studio_model_id = model_before_switch

    raise HTTPException(
        status_code=422, detail=f"[{req_id}] æœªèƒ½åˆ‡æ¢åˆ°æ¨¡å‹ '{model_id_to_use}'ã€‚è¯·ç¡®ä¿æ¨¡å‹å¯ç”¨ã€‚"
    )


async def _handle_parameter_cache(req_id: str, context: RequestContext) -> None:
    """ä»£ç†åˆ° model_switching.handle_parameter_cache"""
    await ms_param_cache(req_id, context)


async def _prepare_and_validate_request(
    req_id: str,
    request: ChatCompletionRequest,
    check_client_disconnected: Callable,
) -> Tuple[str, List[str], Optional[str]]:
    """å‡†å¤‡å’ŒéªŒè¯è¯·æ±‚ï¼Œè¿”å› (ç»„åˆæç¤º, å›¾ç‰‡è·¯å¾„åˆ—è¡¨, å‡½æ•°å®šä¹‰JSON)ã€‚"""
    try:
        validate_chat_request(request.messages, req_id)
    except ValueError as e:
        raise bad_request(req_id, f"æ— æ•ˆè¯·æ±‚: {e}")

    # ç›´æ¥ä¼ é€’æ¶ˆæ¯åˆ—è¡¨ï¼Œä¸å†æ‰‹åŠ¨åˆ†ç¦»å’Œå‰ç½®æœ«å°¾çš„å·¥å…·ç»“æœ
    # æ»¡è¶³ç”¨æˆ·æ–°éœ€æ±‚ï¼šmake that tool call result where it should be, under that tool call function request in chat history
    messages_to_process = list(request.messages) if request.messages else []

    prepared_prompt, images_list, functions_json = prepare_combined_prompt(
        messages_to_process,
        req_id,
        getattr(request, "tools", None),
        getattr(request, "tool_choice", None),
    )

    # åŸºäº tools/tool_choice çš„ä¸»åŠ¨å‡½æ•°æ‰§è¡Œï¼ˆæ”¯æŒ per-request MCP ç«¯ç‚¹ï¼‰
    try:
        # å°† mcp_endpoint æ³¨å…¥ utils.maybe_execute_tools çš„æ³¨å†Œé€»è¾‘
        if hasattr(request, "mcp_endpoint") and request.mcp_endpoint:
            from .tools_registry import register_runtime_tools

            register_runtime_tools(
                getattr(request, "tools", None), request.mcp_endpoint
            )
        tool_exec_results: Optional[List[Dict[str, Any]]] = await maybe_execute_tools(
            request.messages, request.tools, getattr(request, "tool_choice", None)
        )
    except Exception:
        tool_exec_results = None
    check_client_disconnected("After Prompt Prep")
    # å°†ç»“æœå†…è”åˆ°æç¤ºæœ«å°¾ï¼Œä¾›ç½‘é¡µç«¯ä¸€å¹¶æäº¤
    if tool_exec_results:
        try:
            for res in tool_exec_results:
                name = res.get("name")
                args = res.get("arguments")
                result_str = res.get("result")
                prepared_prompt += (
                    f"\n---\nå·¥å…·æ‰§è¡Œ: {name}\nå‚æ•°:\n{args}\nç»“æœ:\n{result_str}\n"
                )
        except Exception:
            pass
    # è‹¥é…ç½®ä»…æ”¶é›†å½“å‰ç”¨æˆ·æ¶ˆæ¯é™„ä»¶ï¼Œåˆ™åœ¨æ­¤è¿‡æ»¤é™„ä»¶
    try:
        if ONLY_COLLECT_CURRENT_USER_ATTACHMENTS:
            latest_user = None
            for msg in reversed(request.messages or []):
                if getattr(msg, "role", None) == "user":
                    latest_user = msg
                    break
            if latest_user is not None:
                filtered: List[str] = []
                from urllib.parse import unquote, urlparse

                from api_utils.utils import extract_data_url_to_local

                # æ”¶é›†è¯¥æ¡ user æ¶ˆæ¯ä¸Šçš„ data:/file:/ç»å¯¹è·¯å¾„ï¼ˆå­˜åœ¨çš„ï¼‰
                # ç»Ÿä¸€ä» messages é™„ä»¶å­—æ®µæŠ½å–
                for key in ("attachments", "images", "files", "media"):
                    arr = getattr(latest_user, key, None)
                    if not isinstance(arr, list):
                        continue
                    for it in arr:
                        url_value = None
                        if isinstance(it, str):
                            url_value = it
                        elif isinstance(it, dict):
                            url_value = it.get("url") or it.get("path")
                        url_value = (url_value or "").strip()
                        if not url_value:
                            continue
                        if url_value.startswith("data:"):
                            fp = extract_data_url_to_local(url_value)
                            if fp:
                                filtered.append(fp)
                        elif url_value.startswith("file:"):
                            parsed = urlparse(url_value)
                            lp = unquote(parsed.path)
                            if os.path.exists(lp):
                                filtered.append(lp)
                        elif os.path.isabs(url_value) and os.path.exists(url_value):
                            filtered.append(url_value)
                images_list = filtered
    except Exception:
        pass

    return prepared_prompt, images_list, functions_json


async def _handle_response_processing(
    req_id: str,
    request: ChatCompletionRequest,
    page: AsyncPage,
    context: RequestContext,
    result_future: Future,
    submit_button_locator: Locator,
    check_client_disconnected: Callable,
) -> Optional[Tuple[Optional[Event], Optional[Locator], Optional[Callable], bool]]:
    """å¤„ç†å“åº”ç”Ÿæˆ"""

    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨è¾…åŠ©æµ
    from config import get_environment_variable

    stream_port = get_environment_variable("STREAM_PORT")
    use_stream = stream_port != "0"

    if use_stream:
        return await _handle_auxiliary_stream_response(
            req_id,
            request,
            context,
            result_future,
            submit_button_locator,
            check_client_disconnected,
        )
    else:
        return await _handle_playwright_response(
            req_id,
            request,
            page,
            context,
            result_future,
            submit_button_locator,
            check_client_disconnected,
        )


async def _handle_auxiliary_stream_response(
    req_id: str,
    request: ChatCompletionRequest,
    context: RequestContext,
    result_future: Future,
    submit_button_locator: Locator,
    check_client_disconnected: Callable,
) -> Optional[Tuple[Optional[Event], Optional[Locator], Optional[Callable], bool]]:
    """è¾…åŠ©æµå“åº”å¤„ç†è·¯å¾„ï¼šè´Ÿè´£å°† STREAM_QUEUE çš„æ•°æ®è½¬æ¢ä¸º OpenAI å…¼å®¹ SSE/JSONã€‚

    - æµå¼æ¨¡å¼ï¼šè¿”å› StreamingResponseï¼Œé€æ­¥æ¨é€ delta ä¸æœ€ç»ˆ usageã€‚
    - éæµå¼æ¨¡å¼ï¼šèšåˆæœ€ç»ˆå†…å®¹ä¸å‡½æ•°è°ƒç”¨ï¼Œè¿”å› JSONResponseã€‚
    """
    from server import logger

    is_streaming: bool = request.stream or False
    current_ai_studio_model_id = context.get("current_ai_studio_model_id")

    # å…¼å®¹æ—§é€»è¾‘çš„éšæœºIDå‡½æ•°ç§»é™¤ï¼Œç»Ÿä¸€ä½¿ç”¨ _random_id()

    if is_streaming:
        try:
            completion_event = Event()
            # ä½¿ç”¨ç”Ÿæˆå™¨ä½œä¸ºå“åº”ä½“ï¼Œäº¤ç”± FastAPI è¿›è¡Œ SSE æ¨é€
            stream_gen_func = gen_sse_from_aux_stream(
                req_id,
                request,
                current_ai_studio_model_id or MODEL_NAME,
                check_client_disconnected,
                completion_event,
                page=context["page"],
                logger=logger,
            )
            if not result_future.done():
                result_future.set_result(
                    StreamingResponse(stream_gen_func, media_type="text/event-stream")
                )
            else:
                if not completion_event.is_set():
                    completion_event.set()

            return (
                completion_event,
                submit_button_locator,
                check_client_disconnected,
                False,
            )

        except Exception as e:
            logger.error(f"[{req_id}] ä»é˜Ÿåˆ—è·å–æµå¼æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            if completion_event and not completion_event.is_set():
                completion_event.set()
            raise

    else:  # éæµå¼
        content = None
        reasoning_content = None
        functions = None
        final_data_from_aux_stream = None

        # éæµå¼ï¼šæ¶ˆè´¹è¾…åŠ©é˜Ÿåˆ—çš„æœ€ç»ˆç»“æœå¹¶ç»„è£… JSON å“åº”
        async for raw_data in use_stream_response(req_id, page=context["page"]):
            check_client_disconnected(f"éæµå¼è¾…åŠ©æµ - å¾ªç¯ä¸­ ({req_id}): ")

            # ç¡®ä¿ data æ˜¯å­—å…¸ç±»å‹
            if isinstance(raw_data, str):
                try:
                    data = json.loads(raw_data)
                except json.JSONDecodeError:
                    logger.warning(f"[{req_id}] æ— æ³•è§£æéæµå¼æ•°æ®JSON: {raw_data}")
                    continue
            elif isinstance(raw_data, dict):
                data = raw_data
            else:
                logger.warning(f"[{req_id}] éæµå¼æœªçŸ¥æ•°æ®ç±»å‹: {type(raw_data)}")
                continue

            # ç¡®ä¿æ•°æ®æ˜¯å­—å…¸ç±»å‹
            if not isinstance(data, dict):
                logger.warning(f"[{req_id}] éæµå¼æ•°æ®ä¸æ˜¯å­—å…¸ç±»å‹: {data}")
                continue

            final_data_from_aux_stream = data
            if data.get("done"):
                content = data.get("body")
                reasoning_content = data.get("reason")
                functions = data.get("function")
                break

        if (
            final_data_from_aux_stream
            and final_data_from_aux_stream.get("reason") == "internal_timeout"
        ):
            logger.error(f"[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå¤±è´¥: å†…éƒ¨è¶…æ—¶")
            raise HTTPException(status_code=502, detail=f"[{req_id}] è¾…åŠ©æµå¤„ç†é”™è¯¯ (å†…éƒ¨è¶…æ—¶)")

        if (
            final_data_from_aux_stream
            and final_data_from_aux_stream.get("done") is True
            and content is None
        ):
            logger.error(f"[{req_id}] éæµå¼è¯·æ±‚é€šè¿‡è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹")
            raise HTTPException(status_code=502, detail=f"[{req_id}] è¾…åŠ©æµå®Œæˆä½†æœªæä¾›å†…å®¹")

        model_name_for_json = current_ai_studio_model_id or MODEL_NAME
        message_payload = {"role": "assistant", "content": content}
        finish_reason_val = "stop"

        if functions and len(functions) > 0:
            tool_calls_list = []
            for func_idx, function_call_data in enumerate(functions):
                tool_calls_list.append(
                    {
                        "id": f"call_{_random_id()}",
                        "index": func_idx,
                        "type": "function",
                        "function": {
                            "name": function_call_data["name"],
                            "arguments": json.dumps(function_call_data["params"]),
                        },
                    }
                )
            message_payload["tool_calls"] = tool_calls_list
            finish_reason_val = "tool_calls"
            message_payload["content"] = None

        if reasoning_content:
            message_payload["reasoning_content"] = reasoning_content

        usage_stats = calculate_usage_stats(
            [msg.model_dump() for msg in request.messages],
            content or "",
            reasoning_content,
        )

        response_payload = build_chat_completion_response_json(
            req_id,
            model_name_for_json,
            message_payload,
            finish_reason_val,
            usage_stats,
            system_fingerprint="camoufox-proxy",
            seed=request.seed if hasattr(request, "seed") else None,
            response_format=(
                request.response_format if hasattr(request, "response_format") else None
            ),
        )

        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))
        return None, submit_button_locator, check_client_disconnected, (finish_reason_val == "tool_calls")


def _try_parse_tool_calls(content: str) -> Optional[List[Dict[str, Any]]]:
    """å°è¯•ä»å†…å®¹ä¸­è§£æå·¥å…·è°ƒç”¨ JSONã€‚"""
    if not content:
        return None

    clean_text = content.strip()
    # å¤„ç† markdown ä»£ç å—
    if clean_text.startswith("```json"):
        clean_text = clean_text[7:]
    elif clean_text.startswith("```"):
        clean_text = clean_text[3:]

    if clean_text.endswith("```"):
        clean_text = clean_text[:-3]

    clean_text = clean_text.strip()

    try:
        data = json.loads(clean_text)
        tool_calls = []

        # å®ƒå¯ä»¥æ˜¯ä¸€ä¸ªåˆ—è¡¨
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict) and (
                    "name" in item or "function_name" in item
                ):
                    # è½¬æ¢ä¸º OpenAI tool_call æ ¼å¼
                    # å‡è®¾ item æ˜¯ {"name": "...", "arguments": {...}}
                    # æˆ–è€… {"function": "...", "parameters": ...}
                    # æˆ‘ä»¬éœ€è¦ç”Ÿæˆä¸€ä¸ªéšæœº id
                    fn_name = item.get("name") or item.get("function_name")
                    fn_args = (
                        item.get("arguments")
                        or item.get("parameters")
                        or item.get("args")
                        or {}
                    )

                    if isinstance(fn_args, dict):
                        fn_args = json.dumps(fn_args)

                    tool_calls.append(
                        {
                            "id": f"call_{_random_id()}",
                            "type": "function",
                            "function": {"name": fn_name, "arguments": str(fn_args)},
                        }
                    )

        # æˆ–è€…æ˜¯ä¸€ä¸ªå•ç‹¬çš„å¯¹è±¡
        elif isinstance(data, dict):
            # Google AI Studio å¯èƒ½ä¼šè¿”å›å•ä¸ªå¯¹è±¡
            if "name" in data or "function_name" in data:
                fn_name = data.get("name") or data.get("function_name")
                fn_args = (
                    data.get("arguments")
                    or data.get("parameters")
                    or data.get("args")
                    or {}
                )
                if isinstance(fn_args, dict):
                    fn_args = json.dumps(fn_args)

                tool_calls.append(
                    {
                        "id": f"call_{_random_id()}",
                        "type": "function",
                        "function": {"name": fn_name, "arguments": str(fn_args)},
                    }
                )

        return tool_calls if tool_calls else None
    except json.JSONDecodeError:
        return None


async def _handle_playwright_response(
    req_id: str,
    request: ChatCompletionRequest,
    page: AsyncPage,
    context: Dict[str, Any],
    result_future: Future,
    submit_button_locator: Locator,
    check_client_disconnected: Callable,
) -> Optional[Tuple[Optional[Event], Optional[Locator], Optional[Callable], bool]]:
    """ä½¿ç”¨Playwrightå¤„ç†å“åº”"""
    from server import logger

    is_streaming: bool = request.stream or False
    current_ai_studio_model_id = context.get("current_ai_studio_model_id")

    await locate_response_elements(page, req_id, logger, check_client_disconnected)

    check_client_disconnected("After Response Element Located: ")

    if is_streaming:
        completion_event = Event()
        stream_gen_func = gen_sse_from_playwright(
            page,
            logger,
            req_id,
            current_ai_studio_model_id or MODEL_NAME,
            request,
            check_client_disconnected,
            completion_event,
        )
        if not result_future.done():
            result_future.set_result(
                StreamingResponse(stream_gen_func, media_type="text/event-stream")
            )

        return completion_event, submit_button_locator, check_client_disconnected, False
    else:
        # ä½¿ç”¨PageControllerè·å–å“åº”
        page_controller = PageController(page, logger, req_id)
        final_content = await page_controller.get_response(check_client_disconnected)

        # å°è¯•è§£æå·¥å…·è°ƒç”¨
        tool_calls = _try_parse_tool_calls(final_content)

        # è®¡ç®—tokenä½¿ç”¨ç»Ÿè®¡
        usage_stats = calculate_usage_stats(
            [msg.model_dump() for msg in request.messages],
            final_content,
            "",  # Playwrightæ¨¡å¼æ²¡æœ‰reasoning content
        )
        logger.info(f"[{req_id}] Playwrightéæµå¼è®¡ç®—çš„tokenä½¿ç”¨ç»Ÿè®¡: {usage_stats}")

        # ç»Ÿä¸€ä½¿ç”¨æ„é€ å™¨ç”Ÿæˆ OpenAI å…¼å®¹å“åº”
        model_name_for_json = current_ai_studio_model_id or MODEL_NAME

        if tool_calls:
            message_payload = {
                "role": "assistant",
                "content": None,
                "tool_calls": tool_calls,
            }
            finish_reason_val = "tool_calls"
            logger.info(f"[{req_id}] âœ… æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨å“åº”ï¼Œå·²è½¬æ¢æ ¼å¼ã€‚")
        else:
            message_payload = {"role": "assistant", "content": final_content}
            finish_reason_val = "stop"

        response_payload = build_chat_completion_response_json(
            req_id,
            model_name_for_json,
            message_payload,
            finish_reason_val,
            usage_stats,
            system_fingerprint="camoufox-proxy",
            seed=request.seed if hasattr(request, "seed") else None,
            response_format=(
                request.response_format if hasattr(request, "response_format") else None
            ),
        )

        if not result_future.done():
            result_future.set_result(JSONResponse(content=response_payload))

        return None, submit_button_locator, check_client_disconnected, (finish_reason_val == "tool_calls")


async def _cleanup_request_resources(
    req_id: str,
    disconnect_check_task: Optional[asyncio.Task],
    completion_event: Optional[Event],
    result_future: Future,
    is_streaming: bool,
) -> None:
    """æ¸…ç†è¯·æ±‚èµ„æº"""
    import shutil

    from config import UPLOAD_FILES_DIR
    from server import logger

    if disconnect_check_task and not disconnect_check_task.done():
        disconnect_check_task.cancel()
        try:
            await disconnect_check_task
        except asyncio.CancelledError:
            pass
        except Exception as task_clean_err:
            logger.error(f"[{req_id}] æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {task_clean_err}")

    logger.info(f"[{req_id}] å¤„ç†å®Œæˆã€‚")

    # æ¸…ç†æœ¬æ¬¡è¯·æ±‚çš„ä¸Šä¼ å­ç›®å½•ï¼Œé¿å…ç£ç›˜ç´¯ç§¯
    try:
        req_dir = os.path.join(UPLOAD_FILES_DIR, req_id)
        if os.path.isdir(req_dir):
            shutil.rmtree(req_dir, ignore_errors=True)
            logger.info(f"[{req_id}] å·²æ¸…ç†è¯·æ±‚ä¸Šä¼ ç›®å½•: {req_dir}")
    except Exception as clean_err:
        logger.warning(f"[{req_id}] æ¸…ç†ä¸Šä¼ ç›®å½•å¤±è´¥: {clean_err}")

    if (
        is_streaming
        and completion_event
        and not completion_event.is_set()
        and (result_future.done() and result_future.exception() is not None)
    ):
        logger.warning(f"[{req_id}] æµå¼è¯·æ±‚å¼‚å¸¸ï¼Œç¡®ä¿å®Œæˆäº‹ä»¶å·²è®¾ç½®ã€‚")
        completion_event.set()


async def _process_request_refactored(
    req_id: str,
    request: ChatCompletionRequest,
    http_request: Request,
    result_future: Future,
) -> Optional[Tuple[Optional[Event], Optional[Locator], Optional[Callable], bool]]:
    """æ ¸å¿ƒè¯·æ±‚å¤„ç†å‡½æ•° - é‡æ„ç‰ˆæœ¬"""

    # ä¼˜åŒ–ï¼šåœ¨å¼€å§‹ä»»ä½•å¤„ç†å‰ä¸»åŠ¨æ£€æµ‹å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€
    from config import get_environment_variable
    from server import logger

    is_connected = await _test_client_connection(req_id, http_request)
    if not is_connected:
        logger.info(f"[{req_id}] âœ… æ ¸å¿ƒå¤„ç†å‰æ£€æµ‹åˆ°å®¢æˆ·ç«¯æ–­å¼€ï¼Œæå‰é€€å‡ºèŠ‚çœèµ„æº")
        if not result_future.done():
            result_future.set_exception(
                HTTPException(status_code=499, detail=f"[{req_id}] å®¢æˆ·ç«¯åœ¨å¤„ç†å¼€å§‹å‰å·²æ–­å¼€è¿æ¥")
            )
        return None

    stream_port = get_environment_variable("STREAM_PORT")
    use_stream = stream_port != "0"
    if use_stream:
        logger.info(f"[{req_id}] ğŸ”§ è¯·æ±‚å¼€å§‹å‰æ¸…ç©ºæµå¼é˜Ÿåˆ—ï¼ˆé˜²æ­¢æ®‹ç•™æ•°æ®ï¼‰...")
        try:
            from api_utils import clear_stream_queue

            await clear_stream_queue()
            logger.info(f"[{req_id}] âœ… æµå¼é˜Ÿåˆ—å·²æ¸…ç©º")
        except Exception as clear_err:
            logger.warning(f"[{req_id}] âš ï¸ æ¸…ç©ºæµå¼é˜Ÿåˆ—æ—¶å‡ºé”™: {clear_err}")

    context: RequestContext = cast(
        RequestContext, await _initialize_request_context(req_id, request)
    )
    context = await _analyze_model_requirements(req_id, context, request)

    (
        client_disconnected_event,
        disconnect_check_task,
        check_client_disconnected,
    ) = await _setup_disconnect_monitoring(req_id, http_request, result_future)

    page = context["page"]
    submit_button_locator = page.locator(SUBMIT_BUTTON_SELECTOR) if page else None
    completion_event = None

    try:
        await _validate_page_status(req_id, context, check_client_disconnected)

        page_controller = PageController(page, context["logger"], req_id)

        await _handle_model_switching(req_id, context, check_client_disconnected)
        await _handle_parameter_cache(req_id, context)

        (
            prepared_prompt,
            image_list,
            functions_json,
        ) = await _prepare_and_validate_request(
            req_id, request, check_client_disconnected
        )
        # é¢å¤–åˆå¹¶é¡¶å±‚ä¸æ¶ˆæ¯çº§ attachments/filesï¼ˆå…¼å®¹å†å²è®°å½•ï¼‰å·²åœ¨ä¸‹æ–¹å¤„ç†ï¼›æ­¤å¤„ç¡®ä¿è·¯å¾„å­˜åœ¨
        try:
            valid_images: List[str] = []
            for p in image_list:
                if isinstance(p, str) and p and os.path.isabs(p) and os.path.exists(p):
                    valid_images.append(p)
            if len(valid_images) != len(image_list):
                from server import logger

                logger.warning(
                    f"[{req_id}] è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„é™„ä»¶è·¯å¾„: {set(image_list) - set(valid_images)}"
                )
            # Reassigning image_list (type: List[Optional[str]]) to valid_images (type: List[str])
            # We need to make sure subsequent usage expects List[str]
            image_list_str: List[str] = valid_images
        except Exception:
            pass
        # å…¼å®¹: é¡¶å±‚ä¸æ¶ˆæ¯çº§é™„ä»¶å­—æ®µåˆå¹¶åˆ°ä¸Šä¼ åˆ—è¡¨ï¼ˆä»… data:/file:/ç»å¯¹è·¯å¾„ï¼‰
        # é™„ä»¶æ¥æºç­–ç•¥ï¼šä»…æ¥å—å½“å‰è¯·æ±‚æ˜¾å¼æä¾›çš„ data:/file:/ç»å¯¹è·¯å¾„ï¼ˆå­˜åœ¨çš„ï¼‰
        try:
            from urllib.parse import unquote, urlparse

            from api_utils.utils import extract_data_url_to_local

            # é¡¶å±‚ attachments
            top_level_atts = getattr(request, "attachments", None)
            if isinstance(top_level_atts, list) and len(top_level_atts) > 0:
                for it in top_level_atts:
                    url_value = None
                    if isinstance(it, str):
                        url_value = it
                    elif isinstance(it, dict):
                        url_value = it.get("url") or it.get("path")
                    url_value = (url_value or "").strip()
                    if not url_value:
                        continue
                    if url_value.startswith("data:"):
                        fp = extract_data_url_to_local(url_value, req_id=req_id)
                        if fp:
                            image_list_str.append(fp)
                    elif url_value.startswith("file:"):
                        parsed = urlparse(url_value)
                        lp = unquote(parsed.path)
                        if os.path.exists(lp):
                            image_list_str.append(lp)
                    elif os.path.isabs(url_value) and os.path.exists(url_value):
                        image_list_str.append(url_value)
            # æ¶ˆæ¯çº§ attachments/images/files/mediaï¼ˆå…¨é‡æ”¶é›†ï¼Œä½†ä»…ä¿ç•™æœ‰æ•ˆæœ¬åœ°/dataï¼‰
            for msg in request.messages or []:
                for key in ("attachments", "images", "files", "media"):
                    arr = getattr(msg, key, None)
                    if not isinstance(arr, list):
                        continue
                    for it in arr:
                        url_value = None
                        if isinstance(it, str):
                            url_value = it
                        elif isinstance(it, dict):
                            url_value = it.get("url") or it.get("path")
                        url_value = (url_value or "").strip()
                        if not url_value:
                            continue
                        if url_value.startswith("data:"):
                            fp = extract_data_url_to_local(url_value, req_id=req_id)
                            if fp:
                                image_list_str.append(fp)
                        elif url_value.startswith("file:"):
                            parsed = urlparse(url_value)
                            lp = unquote(parsed.path)
                            if os.path.exists(lp):
                                image_list_str.append(lp)
                        elif os.path.isabs(url_value) and os.path.exists(url_value):
                            image_list_str.append(url_value)
        except Exception:
            pass

        # ä½¿ç”¨PageControllerå¤„ç†é¡µé¢äº¤äº’
        # æ³¨æ„ï¼šèŠå¤©å†å²æ¸…ç©ºå·²ç§»è‡³é˜Ÿåˆ—å¤„ç†é”é‡Šæ”¾åæ‰§è¡Œ

        await page_controller.adjust_parameters(
            request.model_dump(exclude_none=True),  # ä½¿ç”¨ exclude_none=True é¿å…ä¼ é€’Noneå€¼
            context.get("page_params_cache", {}),
            context.get("params_cache_lock"),
            context.get("model_id_to_use", MODEL_NAME),
            context.get("parsed_model_list", []),
            check_client_disconnected,
        )

        # ä¼˜åŒ–ï¼šåœ¨æäº¤æç¤ºå‰å†æ¬¡æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥ï¼Œé¿å…ä¸å¿…è¦çš„åå°è¯·æ±‚
        check_client_disconnected("æäº¤æç¤ºå‰æœ€ç»ˆæ£€æŸ¥")

        # ç›´æ¥æäº¤ç»„åˆå¥½çš„æç¤ºï¼ˆåŒ…å«å®Œæ•´çš„å†å²è®°å½•å’Œå·¥å…·ç»“æœï¼‰
        # æ— çŠ¶æ€æ¨¡å¼ï¼šæ¯æ¬¡è¯·æ±‚éƒ½æ˜¯ä¸€ä¸ªæ–°çš„èŠå¤©ä¼šè¯ï¼Œå› æ­¤éœ€è¦æäº¤å®Œæ•´çš„ä¸Šä¸‹æ–‡
        await page_controller.submit_prompt(
            prepared_prompt,
            image_list_str,
            functions_json,
            check_client_disconnected,
        )

        # å“åº”å¤„ç†ä»ç„¶éœ€è¦åœ¨è¿™é‡Œï¼Œå› ä¸ºå®ƒå†³å®šäº†æ˜¯æµå¼è¿˜æ˜¯éæµå¼ï¼Œå¹¶è®¾ç½®future
        response_result = await _handle_response_processing(
            req_id,
            request,
            page,
            context,
            result_future,
            submit_button_locator,
            check_client_disconnected,
        )

        if response_result:
            return response_result

        return None, None, None, False

    except ClientDisconnectedError as disco_err:
        context["logger"].info(f"[{req_id}] æ•è·åˆ°å®¢æˆ·ç«¯æ–­å¼€è¿æ¥ä¿¡å·: {disco_err}")
        if not result_future.done():
            result_future.set_exception(
                client_disconnected(req_id, "Client disconnected during processing.")
            )
    except HTTPException as http_err:
        context["logger"].warning(
            f"[{req_id}] æ•è·åˆ° HTTP å¼‚å¸¸: {http_err.status_code} - {http_err.detail}"
        )
        if not result_future.done():
            result_future.set_exception(http_err)
    except PlaywrightAsyncError as pw_err:
        context["logger"].error(f"[{req_id}] æ•è·åˆ° Playwright é”™è¯¯: {pw_err}")
        await save_error_snapshot(f"process_playwright_error_{req_id}")
        if not result_future.done():
            result_future.set_exception(
                upstream_error(req_id, f"Playwright interaction failed: {pw_err}")
            )
    except Exception as e:
        context["logger"].exception(f"[{req_id}] æ•è·åˆ°æ„å¤–é”™è¯¯")
        await save_error_snapshot(f"process_unexpected_error_{req_id}")
        if not result_future.done():
            result_future.set_exception(
                server_error(req_id, f"Unexpected server error: {e}")
            )
    finally:
        await _cleanup_request_resources(
            req_id,
            disconnect_check_task,
            completion_event,
            result_future,
            request.stream,
        )
